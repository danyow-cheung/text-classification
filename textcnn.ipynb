{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Conv1D,MaxPool1D,Flatten,Dropout,Dense,Input\n",
    "from keras.models import Model\n",
    "from sklearn import metrics\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import multiprocessing\n",
    "import jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "后续计算的参数设置\n",
    "'''\n",
    "#数据集路径\n",
    "dataSource = \"data/goods_zh.txt\"\n",
    "stopWordSource = \"data/stopword.txt\"\n",
    "\n",
    "#分词后保留大于等于最低词频的词\n",
    "miniFreq=1\n",
    "\n",
    "#统一输入文本序列的定长，取了所有序列长度的均值。超出将被截断，不足则补0\n",
    "sequenceLength = 200  \n",
    "batchSize=64\n",
    "epochs=10\n",
    "\n",
    "numClasses = 2\n",
    "#训练集的比例\n",
    "rate = 0.8  \n",
    "\n",
    "#生成嵌入词向量的维度\n",
    "embeddingSize = 150\n",
    "\n",
    "#卷积核数\n",
    "numFilters = 30\n",
    "\n",
    "#卷积核大小\n",
    "filterSizes = [2,3,4,5]\n",
    "dropoutKeepProb = 0.5\n",
    "\n",
    "#L2正则系数\n",
    "l2RegLambda = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28766"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "导入语料库，进行分词\n",
    "'''\n",
    "file = open(\"data/goods_zh.txt\",encoding='utf-8') \n",
    "sentences=[]\n",
    "labels=[]\n",
    "for line in file:\n",
    "    temp=line.replace('\\n','').split(',,')\n",
    "    sentences.append(jieba.lcut(temp[0]))\n",
    "    labels.append((temp[1]))\n",
    "file.close()\n",
    "reviews=sentences\n",
    "# sentences存储分词之后的语料库\n",
    "# labels存储0,1\n",
    "# 两者数据类型都是list\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28766"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "直接调用模型\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"model/word2VecModel\"):\n",
    "    print(\"重新分成模型\")\n",
    "    model = word2vec.Word2Vec(sentences,vector_size=embeddingSize,\n",
    "                         min_count=miniFreq,\n",
    "                         window=2,\n",
    "                         workers=multiprocessing.cpu_count(),sg=1,\n",
    "                         epochs=20)\n",
    "    model.save('model/word2VecModel')\n",
    "else:\n",
    "    print(\"直接调用模型\")\n",
    "    model = gensim.models.Word2Vec.load('model/word2VecModel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停用词的字典变量\n",
    "stopWordDict = {}\n",
    "\n",
    "def readStopWord(stopWordPath):\n",
    "    \"\"\"\n",
    "    读取停用词\n",
    "    \"\"\"       \n",
    "    with open(stopWordPath, \"r\",encoding='utf-8') as f:\n",
    "        stopWords = f.read()\n",
    "        stopWordList = stopWords.splitlines()\n",
    "        \n",
    "        # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "        stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "        return (stopWordDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWordDict=readStopWord(stopWordPath=\"data/stopword.txt\")\n",
    "type(stopWordDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainReviews = []\n",
    "trainLabels = []\n",
    "\n",
    "evalReviews = []\n",
    "evalLabels = []\n",
    "\n",
    "wordEmbedding =None\n",
    "n_symbols=0\n",
    "\n",
    "wordToIndex = {}\n",
    "indexToWord = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getWordEmbedding(words):\n",
    "    \"\"\"\n",
    "    按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "    \"\"\"\n",
    "    #中文\n",
    "    model = gensim.models.Word2Vec.load('model/word2VecModel')\n",
    "\n",
    "    vocab = []\n",
    "    wordEmbedding = []\n",
    "\n",
    "    # 添加 \"pad_b\" 和 \"UNK\",  分别表示补齐的用词和未见词 注意，这些词不要在语料中出现\n",
    "    vocab.append(\"pad_b\")\n",
    "    wordEmbedding.append(np.zeros(embeddingSize))\n",
    "\n",
    "    vocab.append(\"UNK\")\n",
    "    wordEmbedding.append(np.random.randn(embeddingSize))\n",
    "    print(\"for循环前的vocab\",vocab)\n",
    "    for word in words:\n",
    "        vector = model.wv[word]\n",
    "        vocab.append(word)\n",
    "        wordEmbedding.append(vector)\n",
    "\n",
    "    print(\"函数内的vocab1\",vocab[:10])\n",
    "    return vocab, np.array(wordEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "\"\"\"\n",
    "\n",
    "allWords = [word for review in reviews for word in review]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去掉停用词\n",
    "subWords = [word for word in allWords if word not in stopWordDict]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#统计词频，排序\n",
    "wordCount = Counter(subWords)  \n",
    "sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除低频词\n",
    "words = [item[0] for item in sortWordCount if item[1] >= miniFreq ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for循环前的vocab ['pad_b', 'UNK']\n",
      "函数内的vocab1 ['pad_b', 'UNK', ' ', '手机', '买', '不错', '京东', '说', '客服', '质量']\n"
     ]
    }
   ],
   "source": [
    "#获取词列表和顺序对应的预训练权重矩阵\n",
    "vocab, wordEmbedding = getWordEmbedding(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pad_b', 'UNK', ' ', '手机', '买', '不错', '京东', '说', '客服', '质量']"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "n_symbols = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断有无json文件\n",
    "if os.path.exists(\"data/wordToIndex.json\"):\n",
    "    pass\n",
    "else:\n",
    "    print(\"存储json文件\")\n",
    "    # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "    with open(\"data/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(wordToIndex, f)\n",
    "\n",
    "\n",
    "if os.path.exists(\"data/indexToWord.json\"):\n",
    "    pass\n",
    "else:\n",
    "    print(\"存储json文件\")\n",
    "    with open(\"data/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(indexToWord, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviewProcess(review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论里面的词，根据词表，映射为index表示\n",
    "        每条评论 用index组成的定长数组来表示\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "生成训练集和验证集\n",
    "\"\"\"\n",
    "\n",
    "reviews_index = []\n",
    "labels_index = []\n",
    "\n",
    "# 遍历所有的文本，将文本中的词转换成index表示\n",
    "for i in range(len(reviews)):\n",
    "\n",
    "    reviewVec = reviewProcess(reviews[i], sequenceLength, wordToIndex)\n",
    "    reviews_index.append(reviewVec)\n",
    "\n",
    "    labels_index.append([labels[i]])\n",
    "\n",
    "trainIndex = int(len(reviews) * rate)\n",
    "\n",
    "\n",
    "#trainReviews = sequence.pad_sequences(reviews[:trainIndex], maxlen=self.sequenceLength)\n",
    "trainReviews = np.asarray(reviews_index[:trainIndex], dtype=\"int64\")\n",
    "trainLabels = np.array(labels_index[:trainIndex], dtype=\"float32\")\n",
    "\n",
    "#evalReviews = sequence.pad_sequences(reviews[trainIndex:], maxlen=self.sequenceLength)\n",
    "evalReviews = np.asarray(reviews_index[trainIndex:], dtype=\"int64\")\n",
    "evalLabels = np.array(labels_index[trainIndex:], dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   1,   1, ...,   0,   0,   0],\n",
       "       [  1,   1,  14, ...,   0,   0,   0],\n",
       "       [152,   1,   1, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [ 25,   5,   1, ...,   0,   0,   0],\n",
       "       [  1, 432,   1, ...,   0,   0,   0],\n",
       "       [142,  86,  47, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainReviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocab)\n",
    "# list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)          [(None, 200)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_9 (Embedding)        (None, 200, 150)     3663300     ['input_10[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_27 (Conv1D)             (None, 200, 256)     115456      ['embedding_9[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_28 (Conv1D)             (None, 200, 256)     153856      ['embedding_9[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_29 (Conv1D)             (None, 200, 256)     192256      ['embedding_9[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling1d_27 (MaxPooling1D  (None, 5, 256)      0           ['conv1d_27[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_28 (MaxPooling1D  (None, 5, 256)      0           ['conv1d_28[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_29 (MaxPooling1D  (None, 5, 256)      0           ['conv1d_29[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 5, 768)       0           ['max_pooling1d_27[0][0]',       \n",
      "                                                                  'max_pooling1d_28[0][0]',       \n",
      "                                                                  'max_pooling1d_29[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_9 (Flatten)            (None, 3840)         0           ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 3840)         0           ['flatten_9[0][0]']              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 2)            7682        ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,132,550\n",
      "Trainable params: 469,250\n",
      "Non-trainable params: 3,663,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "'''\n",
    "模型的搭建\n",
    "'''\n",
    "# 模型结构：词嵌入-卷积池化*3-拼接-全连接-dropout-全连接\n",
    "main_input = Input(shape=(sequenceLength,), dtype='float64')\n",
    "# 词嵌入（使用预训练的词向量）\n",
    "embedder = Embedding(len(vocab) , embeddingSize, input_length=sequenceLength, weights=[wordEmbedding], trainable=False)\n",
    "#embedder = Embedding(len(vocab) + 1, 300, input_length=50, trainable=False)\n",
    "embed = embedder(main_input)\n",
    "# 词窗大小分别为3,4,5\n",
    "cnn1 = Conv1D(256, 3, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn1 = MaxPool1D(pool_size=38)(cnn1)\n",
    "cnn2 = Conv1D(256, 4, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn2 = MaxPool1D(pool_size=37)(cnn2)\n",
    "cnn3 = Conv1D(256, 5, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn3 = MaxPool1D(pool_size=36)(cnn3)\n",
    "# 合并三个模型的输出向量\n",
    "cnn = concatenate([cnn1, cnn2, cnn3], axis=-1)\n",
    "flat = Flatten()(cnn)\n",
    "drop = Dropout(0.2)(flat)\n",
    "main_output = Dense(2, activation='softmax')(drop)\n",
    "model = Model(inputs=main_input, outputs=main_output)\n",
    "one_hot_labels = keras.utils.to_categorical(trainLabels)  # 将标签转换为one-hot编码\n",
    "# 模型展示\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重新训练\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 46s 2s/step - loss: 0.6048 - accuracy: 0.7414\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 47s 2s/step - loss: 0.2569 - accuracy: 0.9008\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 43s 1s/step - loss: 0.2143 - accuracy: 0.9173\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 43s 1s/step - loss: 0.1990 - accuracy: 0.9242\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 46s 2s/step - loss: 0.1876 - accuracy: 0.9288\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 48s 2s/step - loss: 0.1818 - accuracy: 0.9319\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 62s 2s/step - loss: 0.1739 - accuracy: 0.9349\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 49s 2s/step - loss: 0.1671 - accuracy: 0.9375\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 48s 2s/step - loss: 0.1629 - accuracy: 0.9400\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 47s 2s/step - loss: 0.1582 - accuracy: 0.9414\n"
     ]
    }
   ],
   "source": [
    "if  os.path.exists(\"model/Text-Categorization.h5\"):\n",
    "    print(\"调入模型\")\n",
    "    model = keras.models.load_model(\"model/Text-Categorization.h5\")\n",
    "else:\n",
    "    print(\"重新训练\")\n",
    "    # 模型编译\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(trainReviews, one_hot_labels, batch_size=800, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(evalReviews)  # 预测样本属于每个类别的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_labels = np.argmax(result, axis=1)  # 获得最大概率对应的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = list(map(str, result_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 0.9214459506430309\n",
      "平均f1-score: 0.9214503352775644\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('准确率', metrics.accuracy_score(evalLabels, result_labels))\n",
    "print('平均f1-score:', metrics.f1_score(evalLabels, result_labels, average='weighted'))\n",
    "'''\n",
    "准确率 0.9214459506430309\n",
    "平均f1-score: 0.9214503352775644\n",
    "重新训练\n",
    "Epoch 1/10\n",
    "29/29 [==============================] - 46s 2s/step - loss: 0.6048 - accuracy: 0.7414\n",
    "Epoch 2/10\n",
    "29/29 [==============================] - 47s 2s/step - loss: 0.2569 - accuracy: 0.9008\n",
    "Epoch 3/10\n",
    "29/29 [==============================] - 43s 1s/step - loss: 0.2143 - accuracy: 0.9173\n",
    "Epoch 4/10\n",
    "29/29 [==============================] - 43s 1s/step - loss: 0.1990 - accuracy: 0.9242\n",
    "Epoch 5/10\n",
    "29/29 [==============================] - 46s 2s/step - loss: 0.1876 - accuracy: 0.9288\n",
    "Epoch 6/10\n",
    "29/29 [==============================] - 48s 2s/step - loss: 0.1818 - accuracy: 0.9319\n",
    "Epoch 7/10\n",
    "29/29 [==============================] - 62s 2s/step - loss: 0.1739 - accuracy: 0.9349\n",
    "Epoch 8/10\n",
    "29/29 [==============================] - 49s 2s/step - loss: 0.1671 - accuracy: 0.9375\n",
    "Epoch 9/10\n",
    "29/29 [==============================] - 48s 2s/step - loss: 0.1629 - accuracy: 0.9400\n",
    "Epoch 10/10\n",
    "29/29 [==============================] - 47s 2s/step - loss: 0.1582 - accuracy: 0.9414\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"model/Text-Categorization.h5\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
